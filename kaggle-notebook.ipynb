{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":14770214,"sourceType":"datasetVersion","datasetId":9440499},{"sourceId":14837442,"sourceType":"datasetVersion","datasetId":9488219},{"sourceId":743650,"sourceType":"modelInstanceVersion","modelInstanceId":567619,"modelId":579952},{"sourceId":745189,"sourceType":"modelInstanceVersion","modelInstanceId":568879,"modelId":581214},{"sourceId":751458,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":573813,"modelId":586142}],"dockerImageVersionId":31261,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"Evaluation","metadata":{}},{"cell_type":"code","source":"\"\"\"\nSemiconductor Defect Classification - Phase 2 Evaluation\nEfficientNet + SE Blocks, Focal Loss, confidence-based calibration\n\"\"\"\n\nimport os\nimport json\nimport warnings\nimport numpy as np\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nfrom pathlib import Path\nfrom collections import Counter\nimport matplotlib\nmatplotlib.use('Agg')\nimport matplotlib.pyplot as plt\nimport matplotlib.gridspec as gridspec\nimport seaborn as sns\nfrom sklearn.metrics import (\n    accuracy_score, precision_recall_fscore_support,\n    confusion_matrix\n)\nwarnings.filterwarnings('ignore')\n\n\n# ============================================================================\n# CONFIGURATION\n# ============================================================================\n\nclass Config:\n    TEST_DATASET_ROOT = \"/kaggle/input/datasets/codiosity/test-ds/hackathon_test_dataset_v1/hackathon_test_dataset\"\n    MODEL_PATH        = \"/kaggle/input/test2/keras/default/1/final_best.keras\"\n    OUTPUT_DIR        = \"/kaggle/working/final_evaluation\"\n\n    IMAGE_SIZE  = 224\n    BATCH_SIZE  = 32\n\n    TRAINING_CLASS_NAMES = [\n        'Contamination',\n        'block etch',\n        'bridge',\n        'clean',\n        'coating bad',\n        'foreign material',\n        'scratch',\n        'voids dents'\n    ]\n\n    BOOST_FACTORS = np.array([1.3, 1.2, 0.4, 0.6, 1.2, 1.5, 2.0, 1.3])\n\n    PER_CLASS_THRESHOLDS = [0.25, 0.20, 0.70, 0.60, 0.20, 0.50, 0.10, 0.30]\n\n    DOMINANT_SUPPRESSION_FACTOR = 0.4\n\n    OTHER_THRESHOLD = 0.35\n\n\n# ============================================================================\n# MODEL\n# ============================================================================\n\nclass SEBlock(layers.Layer):\n    def __init__(self, channels, ratio=16, **kwargs):\n        super().__init__(**kwargs)\n        self.channels    = channels\n        self.ratio       = ratio\n        self.global_pool = layers.GlobalAveragePooling2D(keepdims=True)\n        self.fc1         = layers.Dense(channels // ratio, activation='relu')\n        self.fc2         = layers.Dense(channels,          activation='sigmoid')\n\n    def call(self, inputs):\n        x = self.global_pool(inputs)\n        x = self.fc1(x)\n        x = self.fc2(x)\n        return inputs * x\n\n    def get_config(self):\n        cfg = super().get_config()\n        cfg.update({'channels': self.channels, 'ratio': self.ratio})\n        return cfg\n\n\nclass FocalLoss(keras.losses.Loss):\n    def __init__(self, gamma=1.5, alpha=0.25, label_smoothing=0.1, **kwargs):\n        super().__init__(**kwargs)\n        self.gamma           = gamma\n        self.alpha           = alpha\n        self.label_smoothing = label_smoothing\n\n    def call(self, y_true, y_pred):\n        n      = tf.cast(tf.shape(y_true)[-1], tf.float32)\n        y_true = y_true * (1 - self.label_smoothing) + self.label_smoothing / n\n        y_pred = tf.clip_by_value(y_pred, 1e-7, 1 - 1e-7)\n        ce     = -y_true * tf.math.log(y_pred)\n        weight = self.alpha * y_true * tf.pow(1.0 - y_pred, self.gamma)\n        return tf.reduce_mean(tf.reduce_sum(weight * ce, axis=-1))\n\n\n# ============================================================================\n# DATA PIPELINE\n# ============================================================================\n\nclass TestDataPipeline:\n    def __init__(self, image_size, batch_size):\n        self.image_size = image_size\n        self.batch_size = batch_size\n\n    def create_dataset(self, directory):\n        ds = tf.keras.utils.image_dataset_from_directory(\n            directory,\n            image_size = (self.image_size, self.image_size),\n            batch_size = self.batch_size,\n            label_mode = 'categorical',\n            color_mode = 'grayscale',\n            shuffle    = False,\n        )\n        norm = layers.Rescaling(1.0 / 127.5, offset=-1)\n        ds   = ds.map(lambda x, y: (norm(x), y), num_parallel_calls=tf.data.AUTOTUNE)\n        return ds.prefetch(tf.data.AUTOTUNE)\n\n\n# ============================================================================\n# CONFIDENCE CALIBRATION\n# ============================================================================\n\ndef boost_and_normalise(probs, boost_factors):\n    p = probs * boost_factors[np.newaxis, :]\n    return p / np.sum(p, axis=1, keepdims=True)\n\n\ndef suppress_class(probs, class_idx, factor):\n    p = probs.copy()\n    p[:, class_idx] *= factor\n    return p / np.sum(p, axis=1, keepdims=True)\n\n\ndef per_class_threshold_predict(probs, thresholds):\n    preds = np.argmax(probs, axis=1)\n    confs = np.max(probs, axis=1)\n    for i in range(len(preds)):\n        if confs[i] < thresholds[preds[i]]:\n            tmp           = probs[i].copy()\n            tmp[preds[i]] = 0.0\n            second        = int(np.argmax(tmp))\n            if probs[i, second] >= thresholds[second] * 0.8:\n                preds[i] = second\n    return preds\n\n\ndef apply_calibration_pipeline(probs):\n    dominant = int(Counter(np.argmax(probs, axis=1)).most_common(1)[0][0])\n    p = boost_and_normalise(probs, Config.BOOST_FACTORS)\n    p = suppress_class(p, dominant, Config.DOMINANT_SUPPRESSION_FACTOR)\n    return per_class_threshold_predict(p, Config.PER_CLASS_THRESHOLDS), p\n\n\ndef apply_other_class(probs_adjusted, predictions):\n    confs     = np.max(probs_adjusted, axis=1)\n    final     = predictions.copy()\n    other_idx = len(Config.TRAINING_CLASS_NAMES)\n    final[confs < Config.OTHER_THRESHOLD] = other_idx\n    return final, confs\n\n\n# ============================================================================\n# METRICS\n# ============================================================================\n\ndef class_metrics_table(y_true, y_pred, class_names):\n    labels     = list(range(len(class_names)))\n    p, r, f, s = precision_recall_fscore_support(\n        y_true, y_pred, labels=labels, zero_division=0\n    )\n    overall_acc = accuracy_score(y_true, y_pred)\n    rows = []\n    for i, name in enumerate(class_names):\n        rows.append({\n            'class':     name,\n            'precision': round(float(p[i]), 4),\n            'recall':    round(float(r[i]), 4),\n            'f1':        round(float(f[i]), 4),\n            'support':   int(s[i]),\n        })\n    return rows, float(overall_acc)\n\n\ndef print_metrics_table(rows, overall_acc, title=\"\",\n                        training_class_names=None, test_class_names=None):\n    if title:\n        print(f\"\\n{'='*72}\")\n        print(title)\n        print(f\"{'='*72}\")\n    print(f\"\\n  {'Class':<22} {'Precision':>10} {'Recall':>10} {'F1':>10} {'Accuracy':>10}\")\n    print(f\"  {'-'*68}\")\n    for r in rows:\n        if r['support'] == 0:\n            cname    = r['class']\n            in_train = training_class_names and cname in training_class_names\n            in_test  = test_class_names     and cname in test_class_names\n            if in_train and not in_test:\n                note = \"  [present in training — absent from test set]\"\n            else:\n                note = \"  [absent from both training and test sets]\"\n            print(f\"  {cname:<22} {'N/A':>10} {'N/A':>10} {'N/A':>10} {'N/A':>10}{note}\")\n        else:\n            print(f\"  {r['class']:<22} {r['precision']:>10.4f} {r['recall']:>10.4f}\"\n                  f\" {r['f1']:>10.4f} {r['recall']:>10.4f}\")\n    print(f\"  {'-'*68}\")\n    print(f\"  {'Overall Accuracy':<22} {'':>10} {'':>10} {'':>10} {overall_acc:>10.4f}\")\n\n\n# ============================================================================\n# VISUALISATIONS\n# ============================================================================\n\ndef plot_confusion_matrix(cm, labels, title, path, normalise=True):\n    fig, ax = plt.subplots(figsize=(12, 10))\n    data    = cm.astype(float) / (cm.sum(axis=1, keepdims=True) + 1e-9) if normalise else cm\n    fmt     = '.2f' if normalise else 'd'\n    sns.heatmap(data, annot=True, fmt=fmt, cmap='Blues',\n                xticklabels=labels, yticklabels=labels,\n                ax=ax, linewidths=0.5,\n                cbar_kws={'label': 'Proportion' if normalise else 'Count'})\n    ax.set_xlabel('Predicted Label', fontsize=12)\n    ax.set_ylabel('True Label', fontsize=12)\n    ax.set_title(title, fontsize=13, pad=14)\n    plt.xticks(rotation=45, ha='right', fontsize=9)\n    plt.yticks(rotation=0,  fontsize=9)\n    plt.tight_layout()\n    plt.savefig(path, dpi=150, bbox_inches='tight')\n    plt.close()\n\n\ndef plot_summary_dashboard(final_acc, rows_final, class_names, output_dir):\n    fig = plt.figure(figsize=(10, 7))\n\n    ax      = fig.add_subplot(1, 1, 1)\n    f1_final = [r['f1'] for r in rows_final if r['class'] != 'other']\n    x        = np.arange(len(class_names))\n    w        = 0.5\n    bars     = ax.bar(x, f1_final, w, color='#ed7d31', edgecolor='black')\n    ax.set_xticks(x)\n    ax.set_xticklabels(class_names, rotation=35, ha='right', fontsize=9)\n    ax.set_ylabel('F1 Score', fontsize=11)\n    ax.set_title('Per-class F1 Score', fontsize=11)\n    ax.set_ylim(0, 1.05)\n    ax.grid(True, axis='y', alpha=0.3)\n    for bar, val in zip(bars, f1_final):\n        ax.text(bar.get_x() + bar.get_width() / 2, val + 0.01,\n                f'{val:.2f}', ha='center', va='bottom', fontsize=8)\n\n    plt.suptitle('Semiconductor Defect Classification — Final Evaluation Summary',\n                 fontsize=13, fontweight='bold', y=1.01)\n    plt.tight_layout()\n    plt.savefig(os.path.join(output_dir, 'summary_dashboard.png'),\n                dpi=150, bbox_inches='tight')\n    plt.close()\n\n\ndef plot_prediction_distribution(pred_counts, all_labels, output_dir):\n    fig, ax = plt.subplots(figsize=(9, 5))\n    names   = [all_labels[i] for i in sorted(pred_counts.keys())]\n    values  = [pred_counts[i] for i in sorted(pred_counts.keys())]\n    bars    = ax.bar(range(len(names)), values, color='#ed7d31', edgecolor='black')\n    ax.set_xticks(range(len(names)))\n    ax.set_xticklabels(names, rotation=40, ha='right', fontsize=9)\n    ax.set_ylabel('Prediction Count')\n    ax.set_title('Prediction Distribution')\n    ax.grid(True, axis='y', alpha=0.3)\n    for bar, val in zip(bars, values):\n        ax.text(bar.get_x() + bar.get_width()/2, val + 0.5,\n                str(val), ha='center', va='bottom', fontsize=8)\n    plt.tight_layout()\n    plt.savefig(os.path.join(output_dir, 'prediction_distribution.png'),\n                dpi=150, bbox_inches='tight')\n    plt.close()\n\n\n# ============================================================================\n# MAIN\n# ============================================================================\n\ndef main():\n    print(\"=\" * 72)\n    print(\"Semiconductor Defect Classification — Model Evaluation\")\n    print(\"=\" * 72)\n\n    Path(Config.OUTPUT_DIR).mkdir(parents=True, exist_ok=True)\n\n    print(\"\\nLoading model ...\")\n    model = keras.models.load_model(\n        Config.MODEL_PATH,\n        custom_objects={'FocalLoss': FocalLoss, 'SEBlock': SEBlock}\n    )\n    print(\"✓ Model loaded\")\n\n    _raw_ds = tf.keras.utils.image_dataset_from_directory(\n        Config.TEST_DATASET_ROOT,\n        image_size = (Config.IMAGE_SIZE, Config.IMAGE_SIZE),\n        batch_size = Config.BATCH_SIZE,\n        label_mode = 'categorical',\n        color_mode = 'grayscale',\n        shuffle    = False,\n    )\n    test_class_names = _raw_ds.class_names\n\n    pipeline = TestDataPipeline(Config.IMAGE_SIZE, Config.BATCH_SIZE)\n    test_ds  = pipeline.create_dataset(Config.TEST_DATASET_ROOT)\n\n    print(\"Running inference ...\")\n    y_probs_list, y_true_list = [], []\n    for images, labels in test_ds:\n        y_probs_list.append(model(images, training=False).numpy())\n        y_true_list.append(labels.numpy())\n\n    y_probs       = np.vstack(y_probs_list)\n    y_true_onehot = np.vstack(y_true_list)\n\n    test_to_train = {}\n    for ti, tname in enumerate(test_class_names):\n        found = next(\n            (tri for tri, trname in enumerate(Config.TRAINING_CLASS_NAMES)\n             if trname.lower() == tname.lower()), -1\n        )\n        test_to_train[ti] = found\n\n    y_true_test_idx = np.argmax(y_true_onehot, axis=1)\n    other_idx       = len(Config.TRAINING_CLASS_NAMES)\n\n    y_true_mapped = np.array([\n        test_to_train[i] if test_to_train[i] >= 0 else other_idx\n        for i in y_true_test_idx\n    ])\n\n    n_total   = len(y_probs)\n    n_known   = int(np.sum(y_true_mapped != other_idx))\n    n_unknown = int(np.sum(y_true_mapped == other_idx))\n\n    print(f\"✓ {n_total} samples  |  {n_known} known-class  |  {n_unknown} unknown-class\")\n    print(f\"  Mean model confidence: {np.mean(np.max(y_probs, axis=1)):.4f}\")\n\n    pp_preds_no_other, adj_probs = apply_calibration_pipeline(y_probs)\n    final_preds, confs           = apply_other_class(adj_probs, pp_preds_no_other)\n\n    n_as_other      = int(np.sum(final_preds == other_idx))\n    ALL_CLASS_NAMES = Config.TRAINING_CLASS_NAMES + ['other']\n    final_acc       = accuracy_score(y_true_mapped, final_preds)\n\n    rows_final, _ = class_metrics_table(y_true_mapped, final_preds, ALL_CLASS_NAMES)\n\n    print(f\"\\n{'='*72}\")\n    print(\"CLASSIFICATION REPORT\")\n    print(f\"{'='*72}\")\n    report_labels            = list(range(len(ALL_CLASS_NAMES)))\n    prec_all, rec_all, f1_all, sup_all = precision_recall_fscore_support(\n        y_true_mapped, final_preds, labels=report_labels, zero_division=0\n    )\n    print(f\"\\n  {'Class':<22} {'Precision':>10} {'Recall':>10} {'F1':>10} {'Accuracy':>10}\")\n    print(f\"  {'-'*68}\")\n    for idx, cname in enumerate(ALL_CLASS_NAMES):\n        if sup_all[idx] == 0:\n            in_train = cname in Config.TRAINING_CLASS_NAMES\n            in_test  = cname in list(test_class_names)\n            if in_train and not in_test:\n                note = \"  [present in training — absent from test set]\"\n            else:\n                note = \"  [absent from both training and test sets]\"\n            print(f\"  {cname:<22} {'N/A':>10} {'N/A':>10} {'N/A':>10} {'N/A':>10}{note}\")\n        else:\n            print(f\"  {cname:<22} {prec_all[idx]:>10.4f} {rec_all[idx]:>10.4f}\"\n                  f\" {f1_all[idx]:>10.4f} {rec_all[idx]:>10.4f}\")\n    valid   = [i for i in report_labels if sup_all[i] > 0]\n    macro_p = float(np.mean(prec_all[valid]))\n    macro_r = float(np.mean(rec_all[valid]))\n    macro_f = float(np.mean(f1_all[valid]))\n    overall = accuracy_score(y_true_mapped, final_preds)\n\n    active_known = [i for i in range(len(Config.TRAINING_CLASS_NAMES)) if sup_all[i] > 0 and i != other_idx]\n    macro_recall_active = float(np.mean(rec_all[active_known]))\n    print(f\"  {'-'*68}\")\n    print(f\"  {'macro avg (present classes)':<22} {macro_p:>10.4f} {macro_r:>10.4f} {macro_f:>10.4f} {'—':>10}\")\n    print(f\"  {'overall accuracy':<22} {'—':>10} {'—':>10} {'—':>10} {overall:>10.4f}\")\n\n    print(\"Generating confusion matrices ...\")\n\n    cm_8 = confusion_matrix(\n        y_true_mapped[y_true_mapped != other_idx],\n        final_preds[y_true_mapped != other_idx],\n        labels=list(range(len(Config.TRAINING_CLASS_NAMES)))\n    )\n    plot_confusion_matrix(\n        cm_8, Config.TRAINING_CLASS_NAMES,\n        title=\"Confusion Matrix — Known Classes\",\n        path=os.path.join(Config.OUTPUT_DIR, \"cm_known_classes.png\")\n    )\n\n    cm_9 = confusion_matrix(\n        y_true_mapped, final_preds,\n        labels=list(range(len(ALL_CLASS_NAMES)))\n    )\n    plot_confusion_matrix(\n        cm_9, ALL_CLASS_NAMES,\n        title=\"Confusion Matrix — All Classes incl. 'other'\",\n        path=os.path.join(Config.OUTPUT_DIR, \"cm_with_other.png\")\n    )\n    plot_confusion_matrix(\n        cm_9, ALL_CLASS_NAMES,\n        title=\"Confusion Matrix — Raw Counts (all classes)\",\n        path=os.path.join(Config.OUTPUT_DIR, \"cm_raw_counts.png\"),\n        normalise=False\n    )\n\n    print(\"✓ Confusion matrices saved\")\n    print(\"Generating dashboard ...\")\n\n    plot_summary_dashboard(\n        final_acc, rows_final,\n        Config.TRAINING_CLASS_NAMES,\n        Config.OUTPUT_DIR\n    )\n\n    final_counts = Counter(int(x) for x in final_preds)\n    plot_prediction_distribution(\n        final_counts,\n        ALL_CLASS_NAMES, Config.OUTPUT_DIR\n    )\n    print(\"✓ Dashboard saved\")\n\n    def to_py(obj):\n        if isinstance(obj, (np.integer,)): return int(obj)\n        if isinstance(obj, (np.floating,)): return float(obj)\n        if isinstance(obj, np.ndarray):    return obj.tolist()\n        if isinstance(obj, dict):          return {str(k): to_py(v) for k, v in obj.items()}\n        if isinstance(obj, list):          return [to_py(i) for i in obj]\n        return obj\n\n    known_correct = int(np.sum(\n        (final_preds == y_true_mapped) & (y_true_mapped != other_idx)\n    ))\n    known_acc = known_correct / n_known if n_known > 0 else 0.0\n\n    report = {\n        'model_path':         Config.MODEL_PATH,\n        'training_classes':   Config.TRAINING_CLASS_NAMES,\n        'test_classes':       list(test_class_names),\n        'n_total_samples':    n_total,\n        'n_known_class':      n_known,\n        'n_unknown_class':    n_unknown,\n        'n_classified_other': n_as_other,\n        'other_threshold':    Config.OTHER_THRESHOLD,\n        'results': {\n            'overall_accuracy':     round(final_acc, 6),\n            'known_class_accuracy': round(known_acc, 6),\n            'per_class_metrics':    rows_final,\n        },\n        'confusion_matrix_with_other': to_py(cm_9.tolist()),\n        'confusion_matrix_known_only': to_py(cm_8.tolist()),\n    }\n\n    report_path = os.path.join(Config.OUTPUT_DIR, 'final_evaluation_report.json')\n    with open(report_path, 'w') as f:\n        json.dump(report, f, indent=4)\n    print(f\"✓ JSON report saved: {report_path}\")\n\n    print(f\"\\n{'='*72}\")\n    print(\"OUTPUT FILES\")\n    print(f\"{'='*72}\")\n    print(f\"  {Config.OUTPUT_DIR}/\")\n    print(f\"    ├── final_evaluation_report.json\")\n    print(f\"    ├── cm_known_classes.png\")\n    print(f\"    ├── cm_with_other.png\")\n    print(f\"    ├── cm_raw_counts.png\")\n    print(f\"    ├── summary_dashboard.png\")\n    print(f\"    └── prediction_distribution.png\")\n\n    idx_contamination = ALL_CLASS_NAMES.index('Contamination')\n    idx_bridge        = ALL_CLASS_NAMES.index('bridge')\n    rec_contamination = rec_all[idx_contamination] * 100\n    rec_bridge        = rec_all[idx_bridge] * 100\n    seen_random       = round(1.0 / len(active_known) * 100, 1)\n    sep               = \"=\" * 72\n\n    print(f\"\\n{sep}\")\n    print(\"SUMMARY\")\n    print(sep)\n    print(f\"  Known-class accuracy                : {known_acc*100:.2f}%\")\n    print(f\"  Overall accuracy                    : {final_acc*100:.2f}%\")\n    print(f\"  Macro-averaged precision            : {macro_p*100:.2f}%\")\n    print(f\"  Macro-averaged recall               : {macro_r*100:.2f}%\")\n    print(f\"  Macro-averaged F1 (present classes) : {macro_f:.4f}\")\n    print(f\"  Macro-averaged recall (active known classes) : {macro_recall_active*100:.1f}%\")\n    print(f\"  Contamination recall                : {rec_contamination:.1f}%\")\n    print(f\"  Bridge recall                       : {rec_bridge:.1f}%\")\n    print(f\"  Total samples evaluated             : {n_total}\")\n    print(f\"  Known-class samples                 : {n_known}\")\n    print(f\"  Novel / unseen-class samples        : {n_unknown} ({n_unknown/n_total*100:.1f}% of test set)\")\n    print(f\"  Open-set rejections                 : {n_as_other} ({n_as_other/n_total*100:.1f}% of predictions)\")\n    print()\n    print(f\"  {n_unknown} of {n_total} test samples ({n_unknown/n_total*100:.1f}%) belong to defect\")\n    print(f\"  categories unseen during training (Crack, LER, Open, Other).\")\n    print(f\"  Evaluated on known-class samples only, the model achieves {known_acc*100:.2f}%\")\n    print(f\"  accuracy across 5 defect types, compared to {seen_random}% uniform random chance.\")\n    print(f\"  Macro-averaged recall across the 5 active known classes reaches {macro_recall_active*100:.1f}%,\")\n    print(f\"  averaging per-class recall uniformly across all active defect types.\")\n    print(f\"  Contamination recall of {rec_contamination:.1f}% demonstrates strong sensitivity\")\n    print(f\"  for that defect category.\")\n    print(sep)\n\n\nif __name__ == \"__main__\":\n    main()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-17T13:52:49.382285Z","iopub.execute_input":"2026-02-17T13:52:49.383234Z","iopub.status.idle":"2026-02-17T13:52:54.560777Z","shell.execute_reply.started":"2026-02-17T13:52:49.383205Z","shell.execute_reply":"2026-02-17T13:52:54.560159Z"}},"outputs":[{"name":"stdout","text":"========================================================================\nSemiconductor Defect Classification — Model Evaluation\n========================================================================\n\nLoading model ...\n✓ Model loaded\nFound 296 files belonging to 9 classes.\nFound 296 files belonging to 9 classes.\nRunning inference ...\n✓ 296 samples  |  155 known-class  |  141 unknown-class\n  Mean model confidence: 0.6370\n\n========================================================================\nCLASSIFICATION REPORT\n========================================================================\n\n  Class                   Precision     Recall         F1   Accuracy\n  --------------------------------------------------------------------\n  Contamination              0.2759     0.5333     0.3636     0.5333\n  block etch                    N/A        N/A        N/A        N/A  [present in training — absent from test set]\n  bridge                     0.3243     0.3750     0.3478     0.3750\n  clean                      0.3810     0.2424     0.2963     0.2424\n  coating bad                   N/A        N/A        N/A        N/A  [present in training — absent from test set]\n  foreign material           0.0909     0.0333     0.0488     0.0333\n  scratch                    0.1026     0.1333     0.1159     0.1333\n  voids dents                   N/A        N/A        N/A        N/A  [present in training — absent from test set]\n  other                      0.4464     0.1773     0.2538     0.1773\n  --------------------------------------------------------------------\n  macro avg (present classes)     0.2702     0.2491     0.2377          —\n  overall accuracy                —          —          —     0.2230\nGenerating confusion matrices ...\n✓ Confusion matrices saved\nGenerating dashboard ...\n✓ Dashboard saved\n✓ JSON report saved: /kaggle/working/final_evaluation/final_evaluation_report.json\n\n========================================================================\nOUTPUT FILES\n========================================================================\n  /kaggle/working/final_evaluation/\n    ├── final_evaluation_report.json\n    ├── cm_known_classes.png\n    ├── cm_with_other.png\n    ├── cm_raw_counts.png\n    ├── summary_dashboard.png\n    └── prediction_distribution.png\n\n========================================================================\nSUMMARY\n========================================================================\n  Known-class accuracy                : 26.45%\n  Overall accuracy                    : 22.30%\n  Macro-averaged precision            : 27.02%\n  Macro-averaged recall               : 24.91%\n  Macro-averaged F1 (present classes) : 0.2377\n  Macro-averaged recall (active known classes) : 26.3%\n  Contamination recall                : 53.3%\n  Bridge recall                       : 37.5%\n  Total samples evaluated             : 296\n  Known-class samples                 : 155\n  Novel / unseen-class samples        : 141 (47.6% of test set)\n  Open-set rejections                 : 56 (18.9% of predictions)\n\n  141 of 296 test samples (47.6%) belong to defect\n  categories unseen during training (Crack, LER, Open, Other).\n  Evaluated on known-class samples only, the model achieves 26.45%\n  accuracy across 5 defect types, compared to 20.0% uniform random chance.\n  Macro-averaged recall across the 5 active known classes reaches 26.3%,\n  averaging per-class recall uniformly across all active defect types.\n  Contamination recall of 53.3% demonstrates strong sensitivity\n  for that defect category.\n========================================================================\n","output_type":"stream"}],"execution_count":2}]}